{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQFrbNN/lbDXqVj//EqaHR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. Environment Setup and Data Unpacking\n","\n","This initial section handles the crucial setup steps for the Colab environment. The process is organized as follows:\n","\n","1.  **Configuration**: Key paths and filenames are defined as variables at the top for easy modification and clarity.\n","2.  **Mount Google Drive**: The Colab environment is connected to the user's Google Drive to access the project's data archive.\n","3.  **Unpack Dataset**: The data (`.zip` archive) is located on Drive and then unpacked into the local Colab filesystem.\n","\n","This approach is chosen for performance reasons, as reading data from the local Colab storage is significantly faster than from Google Drive.\n"],"metadata":{"id":"srLLW4F_6y5x"}},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","\n","# --- 1. Configuration ---\n","# Define all necessary paths and filenames here.\n","GDRIVE_DATA_PATH = \"/content/drive/MyDrive/EgoVisionProject/Data\"\n","ZIP_FILE_NAME    = \"ego4d_data.zip\"\n","LOCAL_DATA_PATH  = \"/content/data\" # Local, temporary workspace\n","\n","print(\"Configuration set.\")\n","\n","# --- 2. Mount Google Drive ---\n","print(\"Connecting to Google Drive...\")\n","drive.mount('/content/drive', force_remount=True)\n","print(\"Drive connected.\")\n","\n","# --- 3. Unpack the Dataset ---\n","gdrive_zip_file = os.path.join(GDRIVE_DATA_PATH, ZIP_FILE_NAME)\n","print(f\"\\nLooking for dataset archive at: {gdrive_zip_file}\")\n","\n","if os.path.exists(gdrive_zip_file):\n","    print(\"Dataset archive found. Unpacking to local storage...\")\n","    os.makedirs(LOCAL_DATA_PATH, exist_ok=True)\n","\n","    # Unzip directly from Drive to the local path.\n","    !unzip -o -q \"{gdrive_zip_file}\" -d \"{LOCAL_DATA_PATH}\"\n","    print(\"Unpacking complete.\")\n","\n","    # Verification Step\n","    print(\"\\n--- Verifying contents of local data storage... ---\")\n","    !ls -lH \"{LOCAL_DATA_PATH}/ego4d_data/v1/annotations\"\n","    print(\"---------------------------------------------\")\n","else:\n","    print(f\"ERROR: Dataset archive not found at the specified path.\")"],"metadata":{"id":"UX2dpBdi8VcH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Load Annotations into a Pandas DataFrame\n","\n","With the data unpacked into the local environment, the next step is to load the `nlq_train.json` annotations file. The raw JSON has a nested structure, which we will flatten to create a structured `pandas` DataFrame. Each row in the DataFrame will represent a single language query, making it the ideal format for subsequent analysis and exploration."],"metadata":{"id":"kFIv8vTO9VS6"}},{"cell_type":"code","source":["import json\n","import pandas as pd\n","\n","annotations_file = os.path.join(LOCAL_DATA_PATH, 'ego4d_data/v1/annotations/nlq_train.json')\n","all_queries = []\n","\n","try:\n","    with open(annotations_file, 'r') as f:\n","        nlq_data = json.load(f)\n","    print(f\"Successfully loaded '{annotations_file}'\")\n","\n","    # Flatten the JSON structure to extract each query and its context\n","    for video in nlq_data.get('videos', []):\n","        for clip in video.get('clips', []):\n","            # Calculate the full duration of the parent clip\n","            clip_duration = clip.get('video_end_sec', 0) - clip.get('video_start_sec', 0)\n","\n","            for annotation in clip.get('annotations', []):\n","                for query in annotation.get('language_queries', []):\n","                    query['video_uid'] = video['video_uid']\n","                    query['clip_uid'] = clip['clip_uid']\n","                    query['clip_duration_sec'] = clip_duration  # Add parent clip's duration\n","                    all_queries.append(query)\n","\n","    # Create the DataFrame\n","    df_queries = pd.DataFrame(all_queries)\n","    print(f\"\\nSuccessfully created DataFrame with {len(df_queries)} queries.\")\n","    display(df_queries.head())\n","\n","except FileNotFoundError:\n","    print(f\"ERROR: Annotation file not found. Please ensure Cell 1 was executed successfully.\")"],"metadata":{"id":"ILol8AZf9IwQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Exploratory Data Analysis (EDA) - Template Distribution\n","\n","As a first step in our exploratory data analysis, we investigate the composition of the training set by analyzing the distribution of query templates. The Ego4D-NLQ benchmark defines 13 query templates across three main categories (Objects, Place, People). By visualizing these counts, we can gain insight into which types of questions are most prevalent in the dataset, which can inform our modeling and data augmentation strategies."],"metadata":{"id":"k2Skv9s19y5d"}},{"cell_type":"code","source":["\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Ensure the DataFrame from the previous cell exists\n","if 'df_queries' in locals():\n","    # Set plot style\n","    sns.set_theme(style=\"whitegrid\")\n","\n","    # Count the occurrences of each template\n","    template_counts = df_queries['template'].value_counts()\n","\n","    plt.figure(figsize=(10, 8)) # Create a figure for the plot\n","\n","    # Create a horizontal bar plot for better readability of labels\n","    sns.barplot(x=template_counts.values, y=template_counts.index, palette=\"viridis\")\n","\n","    plt.title('Distribution of Query Templates in the Training Set', fontsize=16)\n","    plt.xlabel('Number of Queries', fontsize=12)\n","    plt.ylabel('Query Template', fontsize=12)\n","    plt.tight_layout() # Adjust layout to make room for labels\n","    plt.show()\n","\n","    # Also print the exact counts for the report\n","    print(\"\\nQuery counts per template:\")\n","    print(template_counts)\n","\n","else:\n","    print(\"ERROR: DataFrame 'df_queries' not found. Please run Cell 2 first.\")"],"metadata":{"id":"x-pkJ3j99-8a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. EDA - Analysis of Answer Segment Durations\n","\n","Another crucial aspect of the dataset is the temporal duration of the ground-truth \"answer\" segments. This analysis helps us understand whether the model needs to localize short, precise moments or longer, drawn-out activities. We calculate the duration for each query and visualize its distribution using a histogram. This can inform decisions about the model's temporal attention mechanisms and loss function design."],"metadata":{"id":"qcbZGBgw-ErU"}},{"cell_type":"code","source":["import numpy as np\n","\n","if 'df_queries' in locals():\n","    # Calculate the duration of each answer segment\n","    df_queries['answer_duration_sec'] = df_queries['video_end_sec'] - df_queries['video_start_sec']\n","\n","    # --- Plotting the distribution ---\n","    plt.figure(figsize=(12, 6))\n","    sns.histplot(df_queries['answer_duration_sec'], bins=50, kde=True)\n","\n","    # Add a vertical line for the median duration, which is robust to outliers\n","    median_duration = df_queries['answer_duration_sec'].median()\n","    plt.axvline(median_duration, color='red', linestyle='--', linewidth=2, label=f'Median: {median_duration:.2f}s')\n","\n","    plt.title('Distribution of Answer Segment Durations', fontsize=16)\n","    plt.xlabel('Duration (seconds)', fontsize=12)\n","    plt.ylabel('Frequency', fontsize=12)\n","    plt.legend()\n","    # Using a log scale can help visualize long-tail distributions\n","    # plt.xscale('log')\n","    plt.show()\n","\n","    # Display key statistics\n","    print(\"\\nStatistics for Answer Segment Durations (in seconds):\")\n","    display(df_queries['answer_duration_sec'].describe())\n","else:\n","    print(\"ERROR: DataFrame 'df_queries' not found. Please run Cell 2 first.\")"],"metadata":{"id":"71xGVEHt-Kgi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. EDA - Analysis of Input Clip Durations\n","\n","To fully understand the task's complexity, we also analyze the duration of the input video clips that contain the queries. A large discrepancy between the clip duration and the answer duration signifies a more challenging localization task (i.e., finding a \"needle in a haystack\"). We examine the distribution of these clip durations to understand the typical length of context the model must process."],"metadata":{"id":"IuMe2KJu-VbJ"}},{"cell_type":"code","source":["if 'df_queries' in locals():\n","    # To get a true distribution of clip durations, we first need to look at unique clips\n","    unique_clips = df_queries.drop_duplicates(subset=['clip_uid'])\n","\n","    plt.figure(figsize=(12, 6))\n","    sns.histplot(unique_clips['clip_duration_sec'], bins=50, kde=True, color='purple')\n","\n","    median_clip_duration = unique_clips['clip_duration_sec'].median()\n","    plt.axvline(median_clip_duration, color='red', linestyle='--', linewidth=2, label=f'Median: {median_clip_duration:.2f}s')\n","\n","    plt.title('Distribution of Input Clip Durations', fontsize=16)\n","    plt.xlabel('Duration (seconds)', fontsize=12)\n","    plt.ylabel('Frequency (of unique clips)', fontsize=12)\n","    plt.legend()\n","    plt.show()\n","\n","    print(\"\\nStatistics for Input Clip Durations (in seconds):\")\n","    display(unique_clips['clip_duration_sec'].describe())\n","else:\n","    print(\"ERROR: DataFrame 'df_queries' not found. Please run Cell 2 first.\")"],"metadata":{"id":"6qK2TQmh-WxI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6. EDA - Analysis of Answer Segment Position (Ground Truth Distribution)\n","\n","To further understand the dataset's characteristics, we analyze the temporal position of the ground-truth answer segments within their parent video clips. We calculate the normalized start time for each answer (a value between 0.0 and 1.0, where 0.0 is the beginning of the clip and 1.0 is the end).\n","\n","Visualizing the distribution of these normalized start times reveals any potential temporal bias. For instance, a peak near 0.0 would indicate that answers are frequently located at the beginning of the clips, which is valuable information for the model."],"metadata":{"id":"8zHHes2A-uzd"}},{"cell_type":"code","source":["if 'df_queries' in locals():\n","    # Ensure clip duration is not zero to avoid division errors\n","    valid_clips = df_queries[df_queries['clip_duration_sec'] > 0].copy()\n","\n","    # Calculate the normalized start time of the answer within the clip\n","    valid_clips['normalized_start'] = valid_clips['clip_start_sec'] / valid_clips['clip_duration_sec']\n","\n","    # --- Plotting the distribution ---\n","    plt.figure(figsize=(12, 6))\n","    sns.histplot(valid_clips['normalized_start'], bins=50, kde=True, color='teal')\n","\n","    plt.title('Distribution of Normalized Answer Start Times', fontsize=16)\n","    plt.xlabel('Normalized Start Position (0.0 = Clip Start, 1.0 = Clip End)', fontsize=12)\n","    plt.ylabel('Frequency', fontsize=12)\n","    plt.xlim(0, 1) # Ensure the x-axis is bounded between 0 and 1\n","    plt.show()\n","\n","    print(\"\\nStatistics for Normalized Answer Start Times:\")\n","    display(valid_clips['normalized_start'].describe())\n","else:\n","    print(\"ERROR: DataFrame 'df_queries' not found. Please run Cell 2 first.\")"],"metadata":{"id":"YUvubKMH-ybe"},"execution_count":null,"outputs":[]}]}