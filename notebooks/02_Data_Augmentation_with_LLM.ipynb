{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPGWnDK6jMcAtZi1UPfo5YG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 02 - Augmentation, Pre-training, and Fine-tuning Pipeline\n","\n","## Notebook Overview\n","\n","This notebook implements and evaluates a complete pipeline for enhancing a Natural Language Queries (NLQ) model through data augmentation. The entire workflow, central to our project's extension, is divided into three major phases:\n","\n","1.  **Phase I: LLM-Powered Data Augmentation:** We begin by leveraging a Large Language Model (LLM) to generate a new, synthetic training dataset. Starting from the timestamped narrations in Ego4D, we create NLQ-style questions and automatically associate them with precise temporal ground-truth windows. This phase includes a robust data filtering and validation process to ensure the quality of the synthetic data.\n","\n","2.  **Phase II: Pre-training on Augmented Data:** The newly generated dataset is used to pre-train a baseline NLQ model (e.g., VSLNet). The goal of this phase is to teach the model the fundamental patterns of egocentric question-answering on a large and diverse set of synthetic examples, providing it with a powerful head start before it sees any human-annotated data.\n","\n","3.  **Phase III: Fine-tuning on Official Data:** Finally, the model pre-trained on our synthetic data is fine-tuned on the official `nlq_train.json` dataset. This step adapts the generalized knowledge acquired during pre-training to the specific distribution and nuances of the official benchmark data. The ultimate goal is to demonstrate that this pre-training/fine-tuning strategy improves performance compared to training on the official data alone."],"metadata":{"id":"hSCKRazsvYGS"}},{"cell_type":"markdown","source":["## 1. Environment and Data Setup\n","This initial section handles all the necessary setup to prepare our Colab environment. We will mount Google Drive, clone the model repository, install dependencies, and unpack the dataset into the local runtime for fast access."],"metadata":{"id":"TwA6Z9Fsvmw3"}},{"cell_type":"markdown","source":["### 1.1. Mount Google Drive\n","We begin by mounting Google Drive to access our datasets."],"metadata":{"id":"7Db2bD_vvj1O"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zhOiX0gQvh4-","executionInfo":{"status":"ok","timestamp":1750247824862,"user_tz":-120,"elapsed":21621,"user":{"displayName":"Adt Cup","userId":"09040499084418520868"}},"outputId":"4b102146-d7fb-4312-c824-4b1321b47ad1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["### 1.2. Clone Model Repository and Set Directory\n","Next, we clone the `VSLNet_Code` repository and set it as the main working directory for this notebook. This allows us to call scripts directly."],"metadata":{"id":"a49kgnj0vqcR"}},{"cell_type":"code","source":["%%bash\n","# Clone the repository (if it doesn't already exist)\n","if [ ! -d \"VSLNet_Code\" ]; then\n","  git clone https://github.com/pietrogiancristofaro2001/ego4d-nlq-project.git\n","  # We only need the VSLNet_Code folder\n","  mv ego4d-nlq-project/VSLNet_Code .\n","  rm -rf ego4d-nlq-project\n","  echo \"Repository cloned successfully.\"\n","else\n","  echo \"Repository already exists.\"\n","fi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8fKEcEbqvspU","executionInfo":{"status":"ok","timestamp":1750247831898,"user_tz":-120,"elapsed":1178,"user":{"displayName":"Adt Cup","userId":"09040499084418520868"}},"outputId":"6441af2d-15e2-456f-a8bc-3ba659eec360"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Repository cloned successfully.\n"]},{"output_type":"stream","name":"stderr","text":["Cloning into 'ego4d-nlq-project'...\n"]}]},{"cell_type":"code","source":["# Change the notebook's working directory\n","%cd VSLNet_Code\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xQeGwIJhy6iX","executionInfo":{"status":"ok","timestamp":1750247871047,"user_tz":-120,"elapsed":13,"user":{"displayName":"Adt Cup","userId":"09040499084418520868"}},"outputId":"f862deaa-ff70-4f6d-e928-4bd6b1a08582"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/VSLNet_Code\n"]}]},{"cell_type":"markdown","source":["### 1.3. Configure Environment for Augmentation and Pre-training\n","This is the main control cell for the first two phases of our project. It generates a `vars.sh` file **inside the current directory (`VSLNet_Code/`)**. This script defines all paths and parameters needed for data augmentation and for the subsequent pre-training run."],"metadata":{"id":"em30FomVzzqw"}},{"cell_type":"code","source":["# --- Main Configuration ---\n","#We use our best model configuration for data augumentation, but in case we can change just modifying parameters\n","PRETRAIN_MODEL_USED = \"vsl_net\"  # Options: \"vsl_net\", \"vsl_base\"\n","PRETRAIN_FEATURE_TYPE = \"egovlp\" # Options: \"egovlp\", \"omnivore\"\n","PRETRAIN_TEXT_ENCODER = \"bert\"   # Options: \"bert\", \"glove\"\n","\n","# --- Auto-generated settings based on configuration ---\n","if PRETRAIN_FEATURE_TYPE == \"egovlp\":\n","    feature_dir_name = \"egovlp_fp16\"\n","    visual_feature_dim = 256\n","elif PRETRAIN_FEATURE_TYPE == \"omnivore\":\n","    feature_dir_name = \"omnivore_video_swinl_fp16\"\n","    visual_feature_dim = 1536\n","else:\n","    raise ValueError(\"Invalid FEATURE_TYPE selected.\")\n","\n","pretrain_experiment_name = f\"pretrain_{PRETRAIN_MODEL_USED}_{PRETRAIN_FEATURE_TYPE}_{PRETRAIN_TEXT_ENCODER}\"\n","\n","# --- vars.sh content ---\n","vars_sh_content = f\"\"\"\n","#!/bin/bash\n","\n","# --- I. SHARED PATH CONFIGURATION ---\n","export FEATURE_SOURCE_ZIP_PATH=/content/drive/MyDrive/EgoVisionProject/Data\n","export DRIVE_ZIP_FILENAME=ego4d_data.zip\n","export LOCAL_DATA_ROOT=/content/data\n","export EXPERIMENTS_BASE_DIR=$LOCAL_DATA_ROOT/experiments\n","\n","# --- II. DATA AUGMENTATION & PRE-TRAINING SHARED PATHS ---\n","export LOCAL_ANNOTATIONS_DIR=$LOCAL_DATA_ROOT/ego4d_data/v1/annotations\n","export AUGMENTED_JSON_PATH=$LOCAL_ANNOTATIONS_DIR/nlq_train_augmented.json\n","export NARRATION_JSON_PATH=$LOCAL_ANNOTATIONS_DIR/narration.json\n","export LOCAL_VAL_SPLIT=$LOCAL_ANNOTATIONS_DIR/nlq_val.json\n","\n","# --- III. PRE-TRAINING SPECIFIC CONFIGURATION ---\n","export PRETRAIN_EXPERIMENT_NAME={pretrain_experiment_name}\n","export PRETRAIN_MODEL_NAME={PRETRAIN_MODEL_USED}\n","export PRETRAIN_VISUAL_FEATURE_TYPE={PRETRAIN_FEATURE_TYPE}\n","export PRETRAIN_TEXT_ENCODER_TYPE={PRETRAIN_TEXT_ENCODER}\n","export PRETRAIN_VISUAL_FEATURE_DIM={visual_feature_dim}\n","export PRETRAIN_FEATURE_DIR=$LOCAL_DATA_ROOT/ego4d_data/v1/{feature_dir_name}\n","export PRETRAIN_TRAIN_SPLIT=$AUGMENTED_JSON_PATH\n","export PRETRAINED_CHECKPOINT_PATH=$EXPERIMENTS_BASE_DIR/{pretrain_experiment_name}/best.pth\n","\"\"\"\n","\n","# Write the content to vars.sh in the current directory (VSLNet_Code/)\n","with open(\"vars.sh\", \"w\") as f:\n","    f.write(vars_sh_content)\n"],"metadata":{"id":"4VaxWTkoz8TZ","executionInfo":{"status":"ok","timestamp":1750247896813,"user_tz":-120,"elapsed":255,"user":{"displayName":"Adt Cup","userId":"09040499084418520868"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### 1.4. Install Dependencies\n","We install all required Python libraries from the repository's `requirements.txt`"],"metadata":{"id":"HvSCi_FZy78n"}},{"cell_type":"code","source":["%%bash\n","%%capture\n","\n","pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":567},"id":"DpF8awwGzChy","executionInfo":{"status":"error","timestamp":1750247933800,"user_tz":-120,"elapsed":14719,"user":{"displayName":"Adt Cup","userId":"09040499084418520868"}},"outputId":"8487615e-e2f0-4456-aa88-4a21ed565e08"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting numpy==1.22.4 (from -r requirements.txt (line 2))\n","  Downloading numpy-1.22.4.zip (11.5 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.5/11.5 MB 72.3 MB/s eta 0:00:00\n","  Installing build dependencies: started\n","  Installing build dependencies: finished with status 'done'\n","  Getting requirements to build wheel: started\n","  Getting requirements to build wheel: finished with status 'done'\n","  Preparing metadata (pyproject.toml): started\n","  Preparing metadata (pyproject.toml): finished with status 'done'\n"]},{"output_type":"stream","name":"stderr","text":["bash: line 1: fg: no job control\n","ERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\n","ERROR: Could not find a version that satisfies the requirement torch==1.11.0 (from versions: 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1)\n","ERROR: No matching distribution found for torch==1.11.0\n"]},{"output_type":"error","ename":"CalledProcessError","evalue":"Command 'b'%%capture\\n\\npip install -r requirements.txt\\n'' returned non-zero exit status 1.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-3655844633>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%%capture\\n\\npip install -r requirements.txt\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'%%capture\\n\\npip install -r requirements.txt\\n'' returned non-zero exit status 1."]}]},{"cell_type":"markdown","source":["### 1.5. Extract Dataset from Google Drive\n","We use the variables defined in our `vars.sh` file to copy and extract the main dataset from Drive to the local Colab storage."],"metadata":{"id":"ootItA6b0Aez"}},{"cell_type":"code","source":["%%bash\n","\n","source vars.sh\n","\n","# Create local directory and extract data\n","mkdir -p \"$LOCAL_DATA_ROOT\"\n","DRIVE_ZIP_FILE_PATH=\"$FEATURE_SOURCE_ZIP_PATH/$DRIVE_ZIP_FILENAME\"\n","LOCAL_TEMP_ZIP_FILE=\"/content/$DRIVE_ZIP_FILENAME\"\n","\n","if [ -f \"$DRIVE_ZIP_FILE_PATH\" ]; then\n","    echo \"Copying $DRIVE_ZIP_FILENAME...\"\n","    cp \"$DRIVE_ZIP_FILE_PATH\" \"$LOCAL_TEMP_ZIP_FILE\"\n","    echo \"Extracting data...\"\n","    unzip -o -q \"$LOCAL_TEMP_ZIP_FILE\" -d \"$LOCAL_DATA_ROOT\"\n","    rm \"$LOCAL_TEMP_ZIP_FILE\"\n","    echo \"Data setup complete.\"\n","else\n","    echo \"ERROR: File not found at $DRIVE_ZIP_FILE_PATH\"\n","fi"],"metadata":{"id":"MNvg7aDZ0Ebq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.6. Load Metadata and Create Valid Narration Groups\n","This cell performs the core pre-computation for the data augmentation phase. It loads all necessary annotation files, filters out videos that are present in the validation/test sets, and constructs a list of all possible valid groups of `k=5` consecutive narrations that fall within a single video clip."],"metadata":{"id":"nCmZ-oSM4tSq"}},{"cell_type":"code","source":["import json\n","import os\n","import random\n","import uuid\n","from tqdm.auto import tqdm\n","import glob\n","import numpy as np\n","\n","print(\"--- Starting Pre-computation and Filtering ---\")\n","\n","repo_root = \"/content\"\n","local_data_root = os.path.join(repo_root, \"data\")\n","ego4d_json_path = os.path.join(local_data_root, 'ego4d_data', 'ego4d.json')\n","narration_path = os.path.join(local_data_root, 'ego4d_data', 'v1', 'annotations', 'narration.json')\n","val_json_path = os.path.join(local_data_root, 'ego4d_data', 'v1', 'annotations', 'nlq_val.json')\n","test_json_path = os.path.join(local_data_root, 'ego4d_data', 'v1', 'annotations', 'nlq_test_unannotated.json')\n","\n","# We source the feature directory path defined in vars.sh for consistency\n","# A default is provided just in case.\n","feature_dir_path = os.environ.get('PRETRAIN_FEATURE_DIR', os.path.join(local_data_root, 'ego4d_data/v1/egovlp_fp16'))\n","\n","\n","# Load JSON files\n","print(\"Loading core JSON files...\")\n","with open(ego4d_json_path, 'r') as f: ego4d_data = json.load(f)\n","with open(narration_path, 'r') as f: all_narrations_data = json.load(f)\n","print(\"Files loaded successfully.\")\n","\n","\n","# 1. Exclude videos from val/test sets\n","print(\"\\nFiltering out validation/test set videos...\")\n","excluded_video_uids = set()\n","try:\n","    with open(val_json_path, 'r') as f: val_data = json.load(f)\n","    for video in val_data.get('videos', []): excluded_video_uids.add(video['video_uid'])\n","    with open(test_json_path, 'r') as f: test_data = json.load(f)\n","    for video in test_data.get('videos', []): excluded_video_uids.add(video['video_uid'])\n","    print(f\"Found {len(excluded_video_uids)} unique videos to exclude.\")\n","except FileNotFoundError:\n","    print(f\"Warning: Could not find val/test JSON files.\")\n","\n","\n","# 2. Check for existing features\n","print(\"\\nFiltering out videos without pre-extracted features...\")\n","existing_video_ids = {os.path.basename(f).split('.')[0] for f in glob.glob(os.path.join(feature_dir_path, '*.pt'))}\n","print(f\"Found {len(existing_video_ids)} videos with features.\")\n","\n","\n","# 3. Create a lookup map for clips for efficient access\n","print(\"\\nCreating clip lookup maps...\")\n","all_clips_map = {clip['clip_uid']: clip for clip in ego4d_data.get('clips', [])}\n","video_to_clips_map = {}\n","for clip in ego4d_data.get('clips', []):\n","    vid_uid = clip.get('video_uid')\n","    if vid_uid not in video_to_clips_map: video_to_clips_map[vid_uid] = []\n","    video_to_clips_map[vid_uid].append(clip)\n","print(\"Lookup maps created.\")\n","\n","\n","# 4. Create all possible valid narration groups\n","print(\"\\nConstructing valid narration groups...\")\n","k_narrations = 5\n","all_valid_groups = []\n","for video_uid, video_content in tqdm(all_narrations_data.items(), desc=\"Processing Videos\"):\n","    # Apply all filters\n","    if video_uid in excluded_video_uids or video_uid not in existing_video_ids: continue\n","\n","    clips_for_this_video = video_to_clips_map.get(video_uid, [])\n","    if not clips_for_this_video: continue\n","\n","    narrations_list = video_content.get(\"narration_pass_1\", {}).get(\"narrations\", [])\n","    if len(narrations_list) < k_narrations: continue\n","\n","    # Sort narrations by time to be safe\n","    narrations_list.sort(key=lambda x: x['timestamp_sec'])\n","\n","    # Iterate through all possible consecutive groups of size k\n","    for i in range(len(narrations_list) - k_narrations + 1):\n","        current_group = narrations_list[i : i + k_narrations]\n","        group_start_time = current_group[0]['timestamp_sec']\n","        group_end_time = current_group[-1]['timestamp_sec']\n","\n","        # This is the key check: ensure the group is fully contained in a single parent clip\n","        parent_clip = next((c for c in clips_for_this_video if c['video_start_sec'] <= group_start_time and c['video_end_sec'] >= group_end_time), None)\n","\n","        if parent_clip:\n","            all_valid_groups.append({\n","                \"video_uid\": video_uid,\n","                \"narrations\": current_group,\n","                \"parent_clip_uid\": parent_clip['clip_uid']\n","            })\n","\n","print(f\"\\nPreprocessing complete. Found {len(all_valid_groups)} total valid groups.\")"],"metadata":{"id":"3nfX23vW43X2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Timestamp Window Analysis & Debugging\n","This is a critical step. Before running the expensive LLM, we must ensure our logic for creating timestamp windows is robust. In this section, we will analyze the `window_duration` calculation and verify that it produces valid, non-collapsing time intervals. We will experiment with the formula to find a stable configuration."],"metadata":{"id":"rF5TA1bP4C2l"}},{"cell_type":"code","source":["# --- DEBUGGING SCRIPT ---\n","# Our \"laboratory\" to inspect the calculated values without calling the LLM.\n","\n","print(\"--- Starting Timestamp Debugging Analysis ---\")\n","\n","# Parameters from the EgoVLP paper\n","alpha = 4.9\n","# NEW PARAMETER: Let's define a minimum duration to prevent windows from collapsing.\n","MIN_WINDOW_DURATION_SEC = 1.0\n","\n","# Pre-compute Beta map (average time between narrations per video)\n","video_to_beta_map = {}\n","for video_uid, video_content in all_narrations_data.items():\n","    if video_uid in excluded_video_uids or video_uid not in existing_video_ids: continue\n","    narrations_list = video_content.get(\"narration_pass_1\", {}).get(\"narrations\", [])\n","    if len(narrations_list) < 2: continue\n","    narrations_list.sort(key=lambda x: x['timestamp_sec'])\n","    diffs = [narrations_list[i+1]['timestamp_sec'] - narrations_list[i]['timestamp_sec'] for i in range(len(narrations_list)-1)]\n","    positive_diffs = [d for d in diffs if d > 0]\n","    if positive_diffs: video_to_beta_map[video_uid] = np.mean(positive_diffs)\n","\n","# --- Analysis Loop ---\n","num_groups_to_inspect = 10 # Let's inspect a few random groups\n","successful_windows = 0\n","total_narrations_inspected = 0\n","\n","random.shuffle(all_valid_groups) # Process in random order\n","\n","for group_data in all_valid_groups[:num_groups_to_inspect]:\n","    video_uid = group_data['video_uid']\n","    parent_clip_uid = group_data['parent_clip_uid']\n","    parent_clip_info = all_clips_map.get(parent_clip_uid)\n","    beta_i = video_to_beta_map.get(video_uid)\n","\n","    if not parent_clip_info or not beta_i: continue\n","\n","    print(f\"\\n--- Inspecting Group from Video: {video_uid} | Parent Clip: {parent_clip_uid} ---\")\n","    print(f\"Parent Clip Boundaries: [{parent_clip_info['video_start_sec']:.2f}, {parent_clip_info['video_end_sec']:.2f}] | Avg. narration gap (beta): {beta_i:.2f}s\")\n","\n","    for narration_obj in group_data[\"narrations\"]:\n","        total_narrations_inspected += 1\n","        t_i = narration_obj['timestamp_sec']\n","\n","        # Original calculation from EgoVLP\n","        calculated_duration = beta_i / alpha\n","\n","        # Our new robust calculation\n","        window_duration = max(MIN_WINDOW_DURATION_SEC, calculated_duration)\n","\n","        # Calculate and clip the window to the parent clip's boundaries\n","        start_time_abs = max(parent_clip_info['video_start_sec'], t_i - (window_duration / 2))\n","        end_time_abs = min(parent_clip_info['video_end_sec'], t_i + (window_duration / 2))\n","\n","        is_valid = \"✅ VALID\" if start_time_abs < end_time_abs else \"❌ INVALID\"\n","        if start_time_abs < end_time_abs: successful_windows += 1\n","\n","        print(f\"  Narration at {t_i:.2f}s -> \"\n","              f\"Proposed duration: {calculated_duration:.2f}s -> \"\n","              f\"Final duration: {window_duration:.2f}s -> \"\n","              f\"Final Window: [{start_time_abs:.2f}, {end_time_abs:.2f}] -> {is_valid}\")\n","\n","print(f\"\\n--- Analysis Complete ---\")\n","print(f\"Successfully created {successful_windows} valid windows out of {total_narrations_inspected} narrations inspected.\")"],"metadata":{"id":"-YdtFthn4EBK"},"execution_count":null,"outputs":[]}]}