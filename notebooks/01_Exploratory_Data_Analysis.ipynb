{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPK05dLTCfe6nPnARcUKL0v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. Environment Setup and Data Unpacking\n","\n","This initial section handles the crucial setup steps for the Colab environment. The process is organized as follows:\n","\n","1.  **Configuration**: Key paths and filenames are defined as variables at the top for easy modification and clarity.\n","2.  **Mount Google Drive**: The Colab environment is connected to the user's Google Drive to access the project's data archive.\n","3.  **Unpack Dataset**: The data (`.zip` archive) is located on Drive and then unpacked into the local Colab filesystem for improved performance during analysis and training."],"metadata":{"id":"dmAp6WLDipMt"}},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","\n","# --- 1. Configuration ---\n","# Define all necessary paths and filenames here.\n","GDRIVE_PROJECT_PATH = \"/content/drive/MyDrive/EgoVisionProject/Data\" # Main project folder on Google Drive\n","ZIP_FILE_NAME    = \"ego4d_data.zip\"\n","LOCAL_DATA_PATH  = \"/content/data\" # Local temporary workspace in Colab\n","\n","print(\"Configuration set.\")\n","\n","# --- 2. Mount Google Drive ---\n","print(\"Connecting to Google Drive...\")\n","drive.mount('/content/drive', force_remount=True)\n","print(\"Drive connected.\")\n","\n","# --- 3. Unpack the Dataset ---\n","gdrive_zip_file = os.path.join(GDRIVE_PROJECT_PATH, ZIP_FILE_NAME)\n","print(f\"\\nLooking for dataset archive at: {gdrive_zip_file}\")\n","\n","if os.path.exists(gdrive_zip_file):\n","    print(\"Dataset archive found. Unpacking to local storage...\")\n","    os.makedirs(LOCAL_DATA_PATH, exist_ok=True)\n","\n","    # Unzip directly from Drive to the local path.\n","    # -o flag overwrites files without asking, -q flag for quiet mode.\n","    !unzip -o -q \"{gdrive_zip_file}\" -d \"{LOCAL_DATA_PATH}\"\n","    print(\"Unpacking complete.\")\n","\n","    # Verification Step\n","    print(\"\\n--- Verifying contents of the annotations directory: ---\")\n","    !ls -lH \"{LOCAL_DATA_PATH}/ego4d_data/v1/annotations\"\n","    print(\"---------------------------------------------------------\")\n","else:\n","    print(f\"ERROR: Dataset archive not found at '{gdrive_zip_file}'. Please check the path.\")"],"metadata":{"id":"zoGiZaEJiuxZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Exploratory Data Analysis (EDA)\n","\n","This section is dedicated to the exploratory analysis of the Ego4D-NLQ dataset. We will load the training data prepared by the `00_Setup_Environment.ipynb` notebook, examine its structure, and visualize key statistics. The main goals are:\n","\n","1.  **Understand the Data Structure**: Inspect the format of the annotations, including video clips, queries, and response timestamps.\n","2.  **Analyze Query Characteristics**: Investigate the distribution of query templates and the number of words per query.\n","3.  **Analyze Temporal Characteristics**: Examine the duration of video clips and the corresponding answer segments.\n","4.  **Analyze Video Scenarios**: See how query templates are distributed within the most common video scenarios.\n","\n"],"metadata":{"id":"ZwSSA6X0aLWw"}},{"cell_type":"markdown","source":["## 2.1 DataFrame Preparation and Initial Inspection\n","\n","In this first step, we import all the necessary Python libraries for our analysis, including `pandas` for data manipulation and `seaborn` for visualization. We then load the `ego4d_nlq_train.json` annotation file into a pandas DataFrame. Finally, we display the first few rows of the created DataFrame to verify its structure."],"metadata":{"id":"GOeXkCJ1aMQ1"}},{"cell_type":"code","source":["import json\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import os\n","\n","# Set plot style\n","sns.set(style='darkgrid')\n","\n","# --- Correct Path Definition ---\n","# The setup script unpacks the data into '/content/data'.\n","ANNOTATIONS_DIR = \"/content/data/ego4d_data/v1/annotations\"\n","JSON_FILE_NAME = \"nlq_train.json\"\n","json_file_path = os.path.join(ANNOTATIONS_DIR, JSON_FILE_NAME)\n","\n","print(f\"Attempting to load data from: {json_file_path}\")\n","\n","# Check if the file exists before proceeding\n","if not os.path.exists(json_file_path):\n","    print(f\"ERROR: File not found at '{json_file_path}'.\")\n","    print(\"Please ensure that the '00_Setup_Environment.ipynb' notebook has been run successfully and the path is correct.\")\n","else:\n","    # Load the JSON file\n","    with open(json_file_path, 'r') as f:\n","        data = json.load()\n","\n","    records = []\n","    for video_info in data['videos']:\n","        video_uid = video_info['video_uid']\n","        for clip_info in video_info['clips']:\n","            clip_uid = clip_info['clip_uid']\n","            for ann_info in clip_info['annotations']:\n","                language_queries = ann_info.get('language_queries')\n","                if language_queries:\n","                    for query_info in language_queries:\n","                        records.append({\n","                            'video_uid': video_uid,\n","                            'clip_uid': clip_uid,\n","                            'annotation_uid': ann_info['annotation_uid'],\n","                            'query': query_info['query'],\n","                            'template': query_info['template'],\n","                            'start_time': query_info['clip_start_sec'],\n","                            'end_time': query_info['clip_end_sec'],\n","                            'response_start_time': query_info['response_track_start_sec'],\n","                            'response_end_time': query_info['response_track_end_sec'],\n","                        })\n","\n","    df = pd.DataFrame(records)\n","    print(\"DataFrame created successfully.\")\n","    display(df.head())"],"metadata":{"id":"2uG2GY5oaZbY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.2 Analysis of Query Templates\n","\n","Here, we visualize the counts of each query template using a bar chart to clearly see their frequencies.\n"],"metadata":{"id":"wVjyS0GKbp1Z"}},{"cell_type":"code","source":["# Calculate the frequency of each query template\n","template_counts = df['template'].value_counts()\n","\n","# Create the plot\n","plt.figure(figsize=(12, 8))\n","ax = sns.barplot(x=template_counts.index, y=template_counts.values)\n","\n","# Add titles and labels for clarity\n","plt.title('Distribution of Query Templates', fontsize=16)\n","plt.xlabel('Query Template', fontsize=12)\n","plt.ylabel('Number of Queries', fontsize=12)\n","plt.xticks(rotation=45, ha='right') # Rotate labels to prevent overlap\n","\n","# Add the count annotations on top of each bar\n","for p in ax.patches:\n","    ax.annotate(f'{int(p.get_height())}',\n","                (p.get_x() + p.get_width() / 2., p.get_height()),\n","                ha='center', va='center',\n","                xytext=(0, 9),\n","                textcoords='offset points',\n","                fontsize=10)\n","\n","# Ensure everything fits nicely\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"XOLf3leXbyPg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.3 Analysis of Clip and Response Durations\n","\n","We visualize the distributions of the full clip durations and the ground-truth response durations using box plots, annotated with key statistics."],"metadata":{"id":"FDN1127Ub4TY"}},{"cell_type":"code","source":["# Calculate the duration of the full clips and the response segments in seconds\n","df['clip_duration'] = df['end_time'] - df['start_time']\n","df['response_duration'] = df['response_end_time'] - df['response_start_time']\n","\n","# Prepare data for plotting\n","duration_data = df[['clip_duration', 'response_duration']]\n","\n","# Create the figure with two subplots side-by-side\n","fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n","fig.suptitle('Distribution of Clip and Response Durations (in seconds)', fontsize=18)\n","\n","# --- Plot 1: Clip Duration ---\n","sns.boxplot(y=duration_data['clip_duration'], ax=axes[0])\n","axes[0].set_title('Full Clip Duration', fontsize=14)\n","axes[0].set_ylabel('Duration (s)', fontsize=12)\n","\n","# Calculate statistics for Clip Duration\n","mean_clip = duration_data['clip_duration'].mean()\n","median_clip = duration_data['clip_duration'].median()\n","std_clip = duration_data['clip_duration'].std()\n","\n","# Add text annotations for Clip Duration statistics\n","axes[0].text(0.05, 0.95, f'Mean: {mean_clip:.2f}s\\nStd Dev: {std_clip:.2f}s\\nMedian: {median_clip:.2f}s',\n","             transform=axes[0].transAxes, fontsize=12,\n","             verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))\n","\n","\n","# --- Plot 2: Response Duration ---\n","sns.boxplot(y=duration_data['response_duration'], ax=axes[1])\n","axes[1].set_title('Response Span Duration', fontsize=14)\n","axes[1].set_ylabel('Duration (s)', fontsize=12)\n","\n","# Calculate statistics for Response Duration\n","mean_resp = duration_data['response_duration'].mean()\n","median_resp = duration_data['response_duration'].median()\n","std_resp = duration_data['response_duration'].std()\n","\n","# Add text annotations for Response Duration statistics\n","axes[1].text(0.05, 0.95, f'Mean: {mean_resp:.2f}s\\nStd Dev: {std_resp:.2f}s\\nMedian: {median_resp:.2f}s',\n","             transform=axes[1].transAxes, fontsize=12,\n","             verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))\n","\n","\n","# Show the plots\n","plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","plt.show()"],"metadata":{"id":"Ej1dr37RcIUl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.4 Analysis of Scenarios and Templates\n","\n","We now load the `ego4d.json` metadata file to get the official \"scenario\" for each video. We then analyze how query templates are distributed as a percentage within the top 5 most common scenarios."],"metadata":{"id":"HQ2bidGqcx3I"}},{"cell_type":"code","source":["# --- 1. Load Scenario Metadata ---\n","metadata_base_dir = \"/content/data/ego4d_data\"\n","metadata_file_name = \"ego4d.json\"\n","metadata_path = os.path.join(metadata_base_dir, metadata_file_name)\n","print(f\"Loading scenario metadata from: {metadata_path}\")\n","with open(metadata_path, 'r') as f:\n","    metadata = json.load(f)\n","\n","# --- 2. Create and apply mapping ---\n","video_uid_to_scenario = {v['video_uid']: v['scenario'] for v in metadata['videos']}\n","df['scenario'] = df['video_uid'].map(video_uid_to_scenario)\n","\n","# --- 3. Perform the analysis and plotting ---\n","top_scenarios = df['scenario'].value_counts().nlargest(5).index\n","df_top_scenarios = df[df['scenario'].isin(top_scenarios)]\n","scenario_template_dist = pd.crosstab(df_top_scenarios['scenario'],\n","                                     df_top_scenarios['template'],\n","                                     normalize='index') * 100\n","\n","\n","ax = scenario_template_dist.plot(kind='bar', stacked=True, figsize=(14, 8),\n","                                 title='Percentage of Query Templates in Top 5 Scenarios')\n","\n","plt.xlabel('Scenario', fontsize=12)\n","plt.ylabel('Percentage of Queries (%)', fontsize=12)\n","plt.xticks(rotation=0)\n","plt.legend(title='Template', bbox_to_anchor=(1.05, 1), loc='upper left')\n","plt.tight_layout(rect=[0, 0, 0.85, 1])\n","plt.show()"],"metadata":{"id":"D_h9w2eOc9Dz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.5 Analysis of Query Length\n","\n","Finally, we analyze the complexity of the natural language queries themselves. A simple and effective metric for this is the number of words in each query. This analysis helps us understand the verbosity of the questions our model must interpret. We will create a new column in our DataFrame to store the word count for each query and then visualize the distribution of these lengths using a histogram."],"metadata":{"id":"-aXr06teckSP"}},{"cell_type":"code","source":["df['query_length'] = df['query'].apply(lambda x: len(x.split()))\n","\n","plt.figure(figsize=(10, 6))\n","\n","# Correction: The original code uses plt.hist, not sns.histplot.\n","plt.hist(df['query_length'], bins=20, color='darkviolet', alpha=0.7, rwidth=0.85)\n","plt.grid(axis='y', alpha=0.75) # Correction: Original grid is only on y-axis\n","\n","plt.title('Distribution of Number of Words per Query', fontsize=16)\n","plt.xlabel('Number of Words', fontsize=12)\n","plt.ylabel('Number of Queries', fontsize=12)\n","plt.show()\n","\n","display(df['query_length'].describe())"],"metadata":{"id":"ItXF93jag3aU"},"execution_count":null,"outputs":[]}]}