{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOGzn8vSZYpDwRrThEB49FL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#  Model Training and Evaluation\n","\n","## Notebook Overview\n","\n","This notebook provides a comprehensive and reproducible workflow for training and evaluating VSLNet and VSLBase models on the Ego4D Natural Language Queries (NLQ) task. The process is designed to be run in a Google Colab environment and is broken down into three main sections:\n","\n","1.  **Experiment Set-up:** This section handles all the initial configuration. It mounts Google Drive, sets up a dynamic configuration for our experiments (allowing easy selection of models, features, and encoders), installs necessary dependencies, clones the model's source code, and unpacks the dataset into the local Colab environment for fast access.\n","\n","2.  **Symbolic Links & Data Preparation:** Here, we prepare the data for training. This involves creating symbolic links to point the training scripts to the correct data and feature directories. We then run the `prepare_ego4d_dataset.py` script, which preprocesses the annotation files into the format required by the model's data loaders.\n","\n","3.  **Training:** The final section is dedicated to launching the model training. We will show the command that executes the `main.py` script, using the parameters defined in our configuration, to start a training run. By changing the configuration in Section 1, this command can be used to reproduce any of the experiments documented in our report."],"metadata":{"id":"kG2SVDhry6EE"}},{"cell_type":"markdown","source":["## 1. Experiment Set-up"],"metadata":{"id":"VAUjh7Usy9FS"}},{"cell_type":"markdown","source":["### 1.1. Mount Google Drive\n","We begin by mounting Google Drive to access our datasets and save our experiment results."],"metadata":{"id":"9NdUBW8YzA-R"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"EfVWnrL8zDgx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.2. Define Experiment Configuration\n","This is the main control cell for all our experiments. Modify the variables in this cell to select the model, features, text encoder, and run number. The `vars.sh` script, which controls the entire workflow, will be generated automatically based on these settings."],"metadata":{"id":"eEsKhmeLzHmU"}},{"cell_type":"code","source":["# --- CHOOSE YOUR EXPERIMENT CONFIGURATION ---\n","\n","MODEL_USED = \"vsl_net\"  # Options: \"vsl_net\", \"vsl_base\"\n","FEATURE_TYPE = \"egovlp\" # Options: \"egovlp\", \"omnivore\"\n","TEXT_ENCODER = \"bert\"   # Options: \"bert\", \"glove\"\n","RUN_NUMBER = 1          # An integer for the run number, e.g., 1, 2, 3\n","\n","# --- AUTO-GENERATED SETTINGS ---\n","\n","# Set feature directory and dimension based on FEATURE_TYPE\n","if FEATURE_TYPE == \"egovlp\":\n","    FEATURE_DIR_NAME = \"egovlp_fp16\"\n","    VISUAL_FEATURE_DIM = 256\n","elif FEATURE_TYPE == \"omnivore\":\n","    FEATURE_DIR_NAME = \"omnivore_video_swinl_fp16\"\n","    VISUAL_FEATURE_DIM = 1536\n","else:\n","    raise ValueError(\"Invalid FEATURE_TYPE selected.\")\n","\n","# Construct the unique experiment name\n","EXPERIMENT_NAME = f\"{MODEL_USED}_{FEATURE_TYPE}_{TEXT_ENCODER}_run{RUN_NUMBER}\"\n","\n","# --- GENERATE THE vars.sh SCRIPT CONTENT ---\n","\n","vars_sh_content = f\"\"\"\n","#!/bin/bash\n","\n","# --- Dynamic Experiment Configuration ---\n","export NAME={EXPERIMENT_NAME}\n","export MODEL_NAME={MODEL_USED} # vsl_net or vsl_base\n","export VISUAL_FEATURE_TYPE={FEATURE_TYPE} # egovlp or omnivore\n","export TEXT_ENCODER_TYPE={TEXT_ENCODER} # bert or glove\n","export VISUAL_FEATURE_DIM={VISUAL_FEATURE_DIM}\n","\n","# --- Static Path Configuration ---\n","export FEATURE_SOURCE_ZIP_PATH=/content/drive/MyDrive/EgoVisionProject/Data #change with the directory you have the ego4d_data.zip\n","export DRIVE_ZIP_FILENAME=ego4d_data.zip\n","export MODEL_BASE_DIR=/content/drive/MyDrive/EgoVisionProject/Experiments\n","export LOCAL_DATA_ROOT=/content/data\n","\n","\n","# --- Derived Path Configuration ---\n","export TASK_NAME=nlq_official_v1_$NAME\n","export BASE_DIR=$LOCAL_DATA_ROOT/dataset/$TASK_NAME\n","export FEATURE_BASE_DIR=$LOCAL_DATA_ROOT/features/$TASK_NAME/official\n","export FEATURE_DIR=$LOCAL_DATA_ROOT/ego4d_data/v1/{FEATURE_DIR_NAME}\n","export LOCAL_ANNOTATIONS_DIR=$LOCAL_DATA_ROOT/ego4d_data/v1/annotations\n","export LOCAL_TRAIN_SPLIT=$LOCAL_ANNOTATIONS_DIR/nlq_train.json\n","export LOCAL_VAL_SPLIT=$LOCAL_ANNOTATIONS_DIR/nlq_val.json\n","export LOCAL_TEST_SPLIT=$LOCAL_ANNOTATIONS_DIR/nlq_test_unannotated.json\n","export LOCAL_MODEL_DIR=$LOCAL_DATA_ROOT/experiments\n","\"\"\"\n","\n","# Write the content to the vars.sh file\n","with open(\"vars.sh\", \"w\") as f:\n","    f.write(vars_sh_content)\n","\n","print(\"vars.sh file generated successfully for experiment:\")\n","print(f\"--> {EXPERIMENT_NAME}\")"],"metadata":{"id":"-8-9Q7pYzJ_a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.4. Clone Model Repository\n","Next, we clone the `VSLNet_Code` folder from our GitHub repository. This contains the core Python scripts for the model, training, and evaluation. We then change the current directory to this folder so the scripts can be executed directly."],"metadata":{"id":"U6qcy2Q-0NtA"}},{"cell_type":"code","source":["%%bash\n","\n","# Clone the repository (if it doesn't already exist)\n","if [ ! -d \"VSLNet_Code\" ]; then\n","  git clone https://github.com/pietrogiancristofaro2001/ego4d-nlq-project.git\n","  #The above clones the whole project. Let's move the required code folder\n","  mv ego4d-nlq-project/VSLNet_Code .\n","  rm -rf ego4d-nlq-project\n","  echo \"Repository cloned.\"\n","else\n","  echo \"Repository already exists.\"\n","fi\n"],"metadata":{"id":"KxuCf7Yt0ioU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we change the current directory of the notebook to `VSLNet_Code` using the `%cd` magic command. This ensures that all subsequent cells will be executed from this path, allowing scripts and utilities to be called directly."],"metadata":{"id":"J9C7DfZHXA-d"}},{"cell_type":"code","source":["%cd VSLNet_Code\n","print(f\"Current directory changed to: {%pwd}\")"],"metadata":{"id":"SMdEX2QSXAUl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.5. Install Dependencies\n","We install all the necessary Python libraries for the project. These are listed in the `requirements.txt` file and include packages like PyTorch, Transformers, and others."],"metadata":{"id":"IfcRhbMm0trb"}},{"cell_type":"code","source":["%%bash\n","%%capture\n","\n","source vars.sh\n","\n","pip install pandas matplotlib seaborn nltk submitit torch torchaudio torchvision tqdm transformers tensorboard Pillow terminaltables accelerate bitsandbytes sentencepiece"],"metadata":{"id":"qlV4rmVc0wgm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.6. Unpack Dataset\n","We copy the `ego4d_data.zip` file from Drive to the local Colab storage and unzip it. This significantly speeds up data access during training."],"metadata":{"id":"ip3teglz0zZg"}},{"cell_type":"code","source":["%%bash\n","source vars.sh\n","\n","# Create the local destination directory\n","mkdir -p \"$LOCAL_DATA_ROOT\"\n","\n","# Full path of the zip file on Drive\n","DRIVE_ZIP_FILE_PATH=\"$FEATURE_SOURCE_ZIP_PATH/$DRIVE_ZIP_FILENAME\"\n","# Temporary local path to copy the zip file to\n","LOCAL_TEMP_ZIP_FILE=\"/content/$DRIVE_ZIP_FILENAME\"\n","\n","if [ -f \"$DRIVE_ZIP_FILE_PATH\" ]; then\n","    echo \"Copying $DRIVE_ZIP_FILENAME...\"\n","    cp \"$DRIVE_ZIP_FILE_PATH\" \"$LOCAL_TEMP_ZIP_FILE\"\n","\n","    echo \"Extracting file to $LOCAL_DATA_ROOT...\"\n","    # -o overwrites existing files, -q for quiet mode\n","    unzip -o -q \"$LOCAL_TEMP_ZIP_FILE\" -d \"$LOCAL_DATA_ROOT\"\n","\n","    echo \"Removing temporary zip file...\"\n","    rm \"$LOCAL_TEMP_ZIP_FILE\"\n","\n","    echo \"Data setup complete.\"\n","else\n","    echo \"ERROR: File not found at $DRIVE_ZIP_FILE_PATH\"\n","    exit 1\n","fi"],"metadata":{"id":"q2Q8s1Pg017y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Symbolic Links & Data Preparation"],"metadata":{"id":"5Ahg5Lnl1Bcm"}},{"cell_type":"markdown","source":["### 2.1. Create Symbolic Links\n","The training scripts expect the data and feature directories to be in specific locations. We create symbolic links (`ln -s`) to point from the expected locations to our actual data folders in the local Colab storage. This avoids modifying the core scripts. We create links for the annotations, video features, and the GloVe word embeddings."],"metadata":{"id":"TUg5yx5S1CGw"}},{"cell_type":"code","source":["%%bash\n","source vars.sh\n","\n","# --- Symbolic Link for Data/Annotations ---\n","# The script expects a 'data' folder in the current directory\n","DATA_LINK_TARGET_DIR=\"data\"\n","mkdir -p \"$DATA_LINK_TARGET_DIR\"\n","# Link the 'nlq_official_v1' folder inside it, pointing to our annotations\n","ln -sfn \"$LOCAL_ANNOTATIONS_DIR\" \"$DATA_LINK_TARGET_DIR/nlq_official_v1\"\n","echo \"Created symlink for annotations:\"\n","ls -l \"$DATA_LINK_TARGET_DIR\"\n","\n","\n","# --- Symbolic Link for Video Features ---\n","# The script expects a 'features' folder in the current directory\n","FEATURE_LINK_TARGET_DIR=\"features\"\n","mkdir -p \"$FEATURE_LINK_TARGET_DIR\"\n","# Link the specific feature directory (e.g., egovlp_fp16) inside it\n","ln -sfn \"$FEATURE_DIR\" \"$FEATURE_LINK_TARGET_DIR/$VISUAL_FEATURE_TYPE\"\n","echo -e \"\\nCreated symlink for video features:\"\n","ls -l \"$FEATURE_LINK_TARGET_DIR\"\n","\n","\n","# --- Symbolic Link for GloVe ---\n","# Only create this link if the experiment uses GloVe\n","if [ \"$TEXT_ENCODER_TYPE\" == \"glove\" ]; then\n","  GLOVE_FILE_PATH=\"$LOCAL_DATA_ROOT/ego4d_data/v1/glove_encoder/glove.840B.300d.txt\"\n","  EXPECTED_GLOVE_PARENT_DIR=\"data/features\"\n","  mkdir -p \"$EXPECTED_GLOVE_PARENT_DIR\"\n","  ln -sfn \"$GLOVE_FILE_PATH\" \"$EXPECTED_GLOVE_PARENT_DIR/glove.840B.300d.txt\"\n","  echo -e \"\\nCreated symlink for GloVe embeddings:\"\n","  ls -l \"$EXPECTED_GLOVE_PARENT_DIR\"\n","fi"],"metadata":{"id":"wSQ61omd1GJs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.2. Run Data Preprocessing Script\n","Now we run the `prepare_ego4d_dataset.py` script. This script reads the raw JSON annotation files (`nlq_train.json`, etc.), processes them, and saves them in a format optimized for training. It also pre-processes the text queries using the chosen text encoder (BERT or GloVe)."],"metadata":{"id":"zj4BMkBx1MEd"}},{"cell_type":"code","source":["%%bash\n","source vars.sh\n","\n","echo \"Creating output directories...\"\n","mkdir -p \"$BASE_DIR\"\n","mkdir -p \"$FEATURE_BASE_DIR\"\n","\n","echo \"Running data preparation script...\"\n","python utils/prepare_ego4d_dataset.py \\\n","    --input_train_split \"$LOCAL_TRAIN_SPLIT\" \\\n","    --input_val_split \"$LOCAL_VAL_SPLIT\" \\\n","    --input_test_split \"$LOCAL_TEST_SPLIT\" \\\n","    --video_feature_read_path \"$FEATURE_DIR\" \\\n","    --clip_feature_save_path \"$FEATURE_BASE_DIR\" \\\n","    --output_save_path \"$BASE_DIR\"\n","\n","echo \"Data preparation finished.\""],"metadata":{"id":"f1cIgC9MXWzE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Training"],"metadata":{"id":"jhSn_v64X4q8"}},{"cell_type":"markdown","source":["### 3.1. Launching the Training Script\n","This is the final step. We execute `main.py` with all the configured parameters from `vars.sh`. This command starts the training process for the defined model, features, and hyperparameters. The model checkpoints and prediction results will be saved to your Google Drive in the `Experiments/$NAME` directory."],"metadata":{"id":"wS-Bkb1YYH4S"}},{"cell_type":"code","source":["%%bash\n","source vars.sh\n","\n","# --- Hyper-parameter Configuration ---\n","export DATALOADER_WORKERS=4\n","export NUM_WORKERS=4\n","export BATCH_SIZE=128\n","export DIM=128\n","export NUM_EPOCH=100\n","export MAX_POS_LEN=128\n","export INIT_LR=0.001\n","\n","# --- Construct TensorBoard Log Name ---\n","export TB_LOG_NAME=\"${NAME}_bs${BATCH_SIZE}_dim${DIM}_epoch${NUM_EPOCH}_ilr${INIT_LR}\"\n","\n","# Create local directories for saving models and logs, if they don't exist\n","mkdir -p \"$LOCAL_MODEL_DIR/$NAME\"\n","\n","echo \"--- Starting Training ---\"\n","echo \"Experiment Name: $NAME\"\n","echo \"Model: $MODEL_NAME\"\n","echo \"Video Features: $VISUAL_FEATURE_TYPE (Dim: $VISUAL_FEATURE_DIM)\"\n","echo \"Text Encoder: $TEXT_ENCODER_TYPE\"\n","echo \"--------------------------\"\n","\n","python main.py \\\n","    --task $TASK_NAME \\\n","    --mode train \\\n","    --predictor $TEXT_ENCODER_TYPE \\\n","    --dim $DIM \\\n","    --video_feature_dim $VISUAL_FEATURE_DIM \\\n","    --max_pos_len $MAX_POS_LEN \\\n","    --init_lr $INIT_LR \\\n","    --epochs $NUM_EPOCH \\\n","    --batch_size $BATCH_SIZE \\\n","    --fv official \\\n","    --num_workers $NUM_WORKERS \\\n","    --data_loader_workers $DATALOADER_WORKERS \\\n","    --model_name $MODEL_NAME \\\n","    --visual_feature $VISUAL_FEATURE_TYPE \\\n","    --model_dir \"$LOCAL_MODEL_DIR/$NAME\" \\\n","    --eval_gt_json \"$LOCAL_VAL_SPLIT\" \\\n","    --log_to_tensorboard $TB_LOG_NAME \\\n","    --tb_log_freq 5 \\\n","    --remove_empty_queries_from train"],"metadata":{"id":"5UZQNEccVg-a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Save Results to Google Drive (Optional)\n","After the training is complete, the model checkpoints and prediction files are stored in the local Colab environment. This final, optional step copies the entire experiment folder from the local storage to your specified directory on Google Drive for permanent storage."],"metadata":{"id":"Hhwsl8Dlmby4"}},{"cell_type":"code","source":["%%bash\n","source vars.sh\n","\n","# Source directory (local)\n","SOURCE_DIR=\"$LOCAL_MODEL_DIR/$NAME\"\n","\n","# Destination directory (on Google Drive)\n","DEST_DIR=\"$MODEL_BASE_DIR\"\n","\n","# Check if the local experiment directory exists\n","if [ -d \"$SOURCE_DIR\" ]; then\n","  echo \"Copying results from $SOURCE_DIR to $DEST_DIR...\"\n","  # Create the base destination directory on Drive if it doesn't exist\n","  mkdir -p \"$DEST_DIR\"\n","  # Copy the entire experiment folder recursively\n","  cp -r \"$SOURCE_DIR\" \"$DEST_DIR\"\n","  echo \"Copy complete!\"\n","  echo \"You can find your results in: $DEST_DIR/$NAME\"\n","else\n","  echo \"ERROR: Source directory $SOURCE_DIR not found. Was the training completed?\"\n","fi"],"metadata":{"id":"T0NCZAZhmdUC"},"execution_count":null,"outputs":[]}]}